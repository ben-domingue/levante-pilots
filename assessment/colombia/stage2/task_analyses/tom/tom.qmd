
```{r load-data}
source(here::here("assessment/colombia/stage2/_setup.R"))

tom <- read_rds(file = here("assessment","colombia","stage2","task_analyses","data","tom_data.rds"))
```

Note that our ToM data structure probably needs some work.

```{r}
tom |>
  select(assessment_stage, corpus_trial_type) |>
  distinct()
  
```


Compute sumscores, removing the irrelevant questions.


```{r}

  # remove q2 (attitude question) for hostile attribution
  # rename reference as false belief
tom_cleaned <- tom |>
  mutate(corpus_trial_type = str_replace_all(corpus_trial_type, "_question",""), 
         corpus_trial_type = ifelse(corpus_trial_type == "audio", "attribution", 
                                    corpus_trial_type), 
         corpus_trial_type = ifelse(corpus_trial_type == "reference", 
                                    "false_belief",
                                    corpus_trial_type)) |>
  filter(!is.na(age)) 
```

```{r}
tom_sumscores <- tom_cleaned  |> 
  group_by(user_id, assessment_stage, corpus_trial_type, age) |>
  summarise(correct = mean(correct)) 

ggplot(tom_sumscores, aes(x = age, y = correct)) + 
  geom_point(alpha = .5) + 
  geom_smooth() +
  ylim(0,1) + 
  facet_wrap(~interaction(corpus_trial_type,assessment_stage))
  
```

```{r}
tom_sumscores |>
  ungroup() |>
  select(-assessment_stage) |>
  pivot_wider(names_from = "corpus_trial_type", values_from = "correct") |>
  select(age, attribution, emotion_reasoning, 
         false_belief, reality_check) |>
  GGally::ggpairs( lower = list(continuous = GGally::wrap("points", alpha = 0.3)))
  
  # ggplot(aes(x = age, y = correct)) + 
  # geom_point(alpha = .5) + 
  # geom_smooth() +
  # ylim(0,1) + 
  # facet_wrap(~corpus_trial_type)

```




# IRT models

```{r}
source(here("assessment","colombia","stage2","irt_helpers.R"))

task_data_coded <- tom_cleaned |>
  mutate(item_id = if_else(!is.na(item_id), item_id, item)) |>
  mutate(item_id = item_id |> str_replace_all("-", "_")) |>
  select(matches("id"), age, corpus_trial_type, item_id, chance, correct, rt, server_timestamp)

# identify too slow/fast RTs
# TODO: check min/max values + why are some RTs NA
min_rt <- 0.5
max_rt <- 50
task_data_rt <- task_data_coded |>
  mutate(rt = as.numeric(rt) / 1000, rt_fast = rt < min_rt, rt_slow = rt > max_rt) |>
  filter(is.na(rt) | rt > 0)

task_data_nested <- task_data_rt |>
  filter(is.na(rt_fast) | !rt_fast, is.na(rt_slow) | !rt_slow) |> # remove too slow/fast RTs
  select(-starts_with("rt")) |> # drop all RT columns
  nest(data = everything(), .by = task_id) # nest data by task


task_data_prepped <- task_data_nested |>
  mutate(data_filtered = map(data, \(df) df |> filter_repeat_runs() |>
                               dedupe_items() |> remove_no_var_items()),
         data_prepped = map(data_filtered, to_mirt_shape))
```

Let's start by just fitting our general set of models. 

```{r}
item_types <- c("Rasch", "2PL") #, "3PL") # set of parameterizations
model_types <- c(1, 2) # set of dimensionalities

# add arguments for model fitting to data
task_data_args <- task_data_prepped |>
  # duplicate rows per dimensionality x parameterization
  expand_grid(model_type = model_types, item_type = item_types) |>
  # generate model string with item constraints + dimensionality
  mutate(model_str = pmap_chr(list(data, data_prepped, item_type, model_type),
                              generate_model_str)) |>
  # pull out chance values
  mutate(guess = map(data_filtered, # TODO: check that this gives correct order
                     \(df) df |> distinct(item_inst, chance) |> pull(chance)))

task_models <- task_data_args |>
  mutate(mod = pmap(list(data_prepped, item_type, model_str, model_type,
                         task_id, guess), fit_mirt))

```
Now let's take a look!

```{r}
# get results
task_results <- task_models |>
  mutate(coefs = map(mod, mirt_coefs),
         scores = pmap(list(mod, data_filtered, data_prepped), mirt_scores),
         aic = map_dbl(mod, mirt_aic))

# best fitting model for each task
task_best <- task_results |>
  group_by(task_id) |>
  filter(aic == min(aic)) |>
  ungroup() |>
  select(task_id, item_type, model_type, coefs, scores)

# scores from best fitting models
task_scores <- task_best |>
  select(task_id, item_type, scores) |>
  unnest(scores) |>
  mutate(item_type = fct_recode(item_type, "1PL" = "Rasch"),
         score_type = glue("ability ({item_type})")) |>
  select(task = task_id, user_id, age, score_type, score = ability)
```

## 2PL ToM

Oh interesting, the 2PL 2 dim is winning for ToM. Let's check it out. 

```{r}
tom_items <- tom_cleaned |>
  select(corpus_trial_type, item_id, answer) |>
  rename(item = item_id) |>
  distinct()

tom_coefs <- task_best$coefs[[1]] |>
  mutate(item = str_sub(item, 1, -3)) |>
  left_join(tom_items)

ggplot(tom_coefs |> 
           pivot_longer(a1:a2, names_to = "dim", values_to = "a"), 
       aes(x = d, y = a, col = corpus_trial_type)) + 
  geom_point(alpha = .5) + 
  facet_wrap(~dim)
```

```{r}
tom_coefs |>
  arrange(desc(a1)) |>
  knitr::kable()
```

To be honest, these numbers seem really big, potentially outlier driven? Unless I'm just interpreting the 2D model weirdly. 


## 1PL ToM

I can't make head or tail of the 2D 2PL so let's look at the 1D 2PL. 

```{r}
tom_coefs <- task_results$coefs[[2]] |>
  mutate(item = str_sub(item, 1, -3)) |>
  left_join(tom_items)

ggplot(tom_coefs, 
       aes(x = -d, y = a1, col = corpus_trial_type)) + 
  geom_point(alpha = .5) +
  ggrepel::geom_label_repel(aes(label = item), size =2) + 
  xlab("Difficulty") + 
  ylab("Discrimination")
```

